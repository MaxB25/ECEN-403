{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f2755ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyodbc\n",
    "# connect_string = ''\n",
    "# try:\n",
    "#     conn = pyodbc.connect('DRIVER={SQL Server};SERVER=10.236.6.111;PORT=1433;DATABASE=Teambuilder;UID=admin;PWD=TeamBuilder;')\n",
    "#     print(\"{c} is working\".format(c=connect_string))\n",
    "# except pyodbc.Error as ex:\n",
    "#     print(\"{c} is not working\".format(c=connect_string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a53065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# ##After connecting to database extract preprocessed information\n",
    "# #Read in ID and resumes using pandas\n",
    "\n",
    "# #This is how to create a two column list using two lists newlist = list(zip(numbers,letters))\n",
    "# #Get data from server\n",
    "# student_id = []\n",
    "# student_resume = []\n",
    "# cursor = conn.cursor()\n",
    "# cursor.execute(\"SELECT ALL * FROM StudentInformation\") #StudentInformation is the name of the table\n",
    "# for row in cursor:\n",
    "#         student_id.append(row.StudentName)\n",
    "#         student_resume(row.Resume)\n",
    "# #Store data in pandas data frame\n",
    "# student_info = list(zip(student_id, student_resume))\n",
    "# df = pd.DataFrame(student_info, columns=['ID', 'Processed Resume'])\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64b18d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the same techniques we used to train to train the model with student names and identites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f95b8116",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting Up the Environment\n",
    "import re #For Preprocessing\n",
    "import pandas as pd #For Data Handling\n",
    "from time import time #To Time the Operations\n",
    "from collections import defaultdict #Use for word frequency\n",
    "\n",
    "import spacy #More Preprocessing\n",
    "\n",
    "import logging #Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CandidateID</th>\n",
       "      <th>resume</th>\n",
       "      <th>cleaned_resume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>candidate_009</td>\n",
       "      <td>CutThroat Media  NLP analyst for user commen...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>candidate_046</td>\n",
       "      <td>HOWARD GOODMAN  F R E S H E R  A N D  N L P ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>candidate_070</td>\n",
       "      <td>Publicis Sapient DATA PROGRAMMER, JAN 2019 -...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>candidate_143</td>\n",
       "      <td>M A T T H E W  H O R R E S JUNIOR DEVELOPER ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     CandidateID                                             resume  \\\n",
       "0  candidate_009    CutThroat Media  NLP analyst for user commen...   \n",
       "1  candidate_046    HOWARD GOODMAN  F R E S H E R  A N D  N L P ...   \n",
       "2  candidate_070    Publicis Sapient DATA PROGRAMMER, JAN 2019 -...   \n",
       "3  candidate_143    M A T T H E W  H O R R E S JUNIOR DEVELOPER ...   \n",
       "\n",
       "  cleaned_resume  \n",
       "0                 \n",
       "1                 \n",
       "2                 \n",
       "3                 "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tika import parser\n",
    "import os\n",
    "df = pd.DataFrame(columns=['CandidateID', 'resume', 'cleaned_resume'])\n",
    "for dirname , _, filenames in os.walk('test'):\n",
    "    for filename in filenames:\n",
    "       path = os.path.join(dirname, filename)\n",
    "       raw = parser.from_file(path)\n",
    "       df = df.append({'CandidateID' : filename[:-4], 'resume' : \" \".join(raw['content'].strip().split('\\n')[1:]), 'cleaned_resume' : \"\"}, \n",
    "                ignore_index = True)\n",
    "   \n",
    "df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanResume(resumeText):\n",
    "    resumeText = re.sub('http\\S+\\s*', ' ', resumeText)  # remove URLs\n",
    "    resumeText = re.sub('RT|cc', ' ', resumeText)  # remove RT and cc\n",
    "    resumeText = re.sub('#\\S+', '', resumeText)  # remove hashtags\n",
    "    resumeText = re.sub('@\\S+', '  ', resumeText)  # remove mentions\n",
    "    resumeText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', resumeText)  # remove punctuations\n",
    "    resumeText = re.sub(r'[^\\x00-\\x7f]',r' ', resumeText) \n",
    "    resumeText = re.sub('\\s+', ' ', resumeText)  # remove extra whitespace\n",
    "    return resumeText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CandidateID</th>\n",
       "      <th>cleaned_resume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>candidate_009</td>\n",
       "      <td>CutThroat Media NLP analyst for user comments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>candidate_046</td>\n",
       "      <td>HOWARD GOODMAN F R E S H E R A N D N L P E N ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>candidate_070</td>\n",
       "      <td>Publicis Sapient DATA PROGRAMMER JAN 2019 TIL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>candidate_143</td>\n",
       "      <td>M A T T H E W H O R R E S JUNIOR DEVELOPER WO...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     CandidateID                                     cleaned_resume\n",
       "0  candidate_009   CutThroat Media NLP analyst for user comments...\n",
       "1  candidate_046   HOWARD GOODMAN F R E S H E R A N D N L P E N ...\n",
       "2  candidate_070   Publicis Sapient DATA PROGRAMMER JAN 2019 TIL...\n",
       "3  candidate_143   M A T T H E W H O R R E S JUNIOR DEVELOPER WO..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_resume'] = df.resume.apply(lambda x: cleanResume(x))\n",
    "df = df.drop('resume', 1)\n",
    "df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\n",
    "\n",
    "def cleaning(doc):\n",
    "    # Lemmatizes and removes stopwords\n",
    "    # doc needs to be a spacy Doc object\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
    "    # if a sentence is only one or two words long,\n",
    "    # the benefit for the training is very small\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CandidateID</th>\n",
       "      <th>cleaned_resume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>candidate_009</td>\n",
       "      <td>CutThroat Media NLP analyst for user comments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>candidate_046</td>\n",
       "      <td>HOWARD GOODMAN F R E S H E R A N D N L P E N ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>candidate_070</td>\n",
       "      <td>Publicis Sapient DATA PROGRAMMER JAN 2019 TIL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>candidate_143</td>\n",
       "      <td>M A T T H E W H O R R E S JUNIOR DEVELOPER WO...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     CandidateID                                     cleaned_resume\n",
       "0  candidate_009   CutThroat Media NLP analyst for user comments...\n",
       "1  candidate_046   HOWARD GOODMAN F R E S H E R A N D N L P E N ...\n",
       "2  candidate_070   Publicis Sapient DATA PROGRAMMER JAN 2019 TIL...\n",
       "3  candidate_143   M A T T H E W H O R R E S JUNIOR DEVELOPER WO..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df['cleaned_resume'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to clean up everything: 0.0 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000)]\n",
    "\n",
    "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:50:43: Note: NumExpr detected 24 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO - 23:50:43: NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CandidateID</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>candidate_009</td>\n",
       "      <td>candidate_009   cutthroat medium nlp analyst u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>candidate_046</td>\n",
       "      <td>candidate_046   howard goodman f r e s h e r n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>candidate_070</td>\n",
       "      <td>candidate_070   publicis sapient datum program...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>candidate_143</td>\n",
       "      <td>candidate_143   m t t h e w h o r r e s junior...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     CandidateID                                              clean\n",
       "0  candidate_009  candidate_009   cutthroat medium nlp analyst u...\n",
       "1  candidate_046  candidate_046   howard goodman f r e s h e r n...\n",
       "2  candidate_070  candidate_070   publicis sapient datum program...\n",
       "3  candidate_143  candidate_143   m t t h e w h o r r e s junior..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = pd.DataFrame(columns=[\"CandidateID\", \"clean\"])\n",
    "df_clean = df[[\"CandidateID\"]].copy()\n",
    "df_clean['clean'] = pd.DataFrame({'clean': txt})\n",
    "df_clean = df_clean.dropna().drop_duplicates()\n",
    "df_clean['clean'] = df['CandidateID'] + \" \" + df_clean['clean']\n",
    "df_clean.shape\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = [row.split() for row in df_clean['clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:50:43: collecting all words and their counts\n",
      "INFO - 23:50:43: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 23:50:43: collected 713 token types (unigram + bigrams) from a corpus of 464 words and 4 sentences\n",
      "INFO - 23:50:43: merged Phrases<713 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 23:50:43: Phrases lifecycle event {'msg': 'built Phrases<713 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000> in 0.00s', 'datetime': '2021-11-14T23:50:43.645383', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "phrases = Phrases(sent, min_count=5, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:50:43: exporting phrases from Phrases<713 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 23:50:43: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<1 phrases, min_count=5, threshold=10.0> from Phrases<713 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000> in 0.00s', 'datetime': '2021-11-14T23:50:43.674411', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['candidate_046', 'howard', 'goodman', 'f', 'r', 'e', 's', 'h', 'e', 'r', 'n', 'd', 'n', 'l', 'p', 'e', 'n', 'g', 'n', 'e', 'e', 'r', 'professional', 'profile', 'fresher', 'absolutely', 'love', 'work', 'vaguely', 'relate', 'machine', 'learning', 'acquaint', 'state', 'art', 'nlp', 'model', 'like', 'bert', 'xlnet', 'gpt', 'work', 'datum', 'exploration', 'proficient', 'skill', 'machine_learn', 'natural', 'language', 'process', 'deep', 'learning', 'sentiment', 'analysis', 'python', 'nltk', 'gpt', 'xlnet', 'text', 'analysis', 'text', 'extraction', 'ocr', 'employment', 'history', 'nlp', 'development', 'team', 'take', 'build', 'different', 'pipeline', 'training', 'evaluation', 'different', 'textual', 'datum', 'nlp', 'developer', 'intern', 'apr', 'nov', 'zynta', 'lab', 'educational', 'history', 'b', 'tech', 'electrical', 'rajiv', 'gandhi', 'memorial', 'university', 'delhi', 'achievement', 'extra', 'curricular', 'tensorflow', 'kera', 'certify', 'project', 'embed', 'device', 'convert', 'asl', 'voice', 'vice', 'versa', 'real', 'time']\n"
     ]
    }
   ],
   "source": [
    "sentences = bigram[sent]\n",
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "285"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datum',\n",
       " 'work',\n",
       " 'nlp',\n",
       " 'machine_learn',\n",
       " 'python',\n",
       " 'project',\n",
       " 'r',\n",
       " 'e',\n",
       " 'skill',\n",
       " 'analysis']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 23:56:24: collecting all words and their counts\n",
      "INFO - 23:56:24: PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "INFO - 23:56:24: collected 285 word types and 4 unique tags from a corpus of 4 examples and 457 words\n",
      "INFO - 23:56:24: Creating a fresh vocabulary\n",
      "INFO - 23:56:24: Doc2Vec lifecycle event {'msg': 'effective_min_count=1 retains 285 unique words (100.0%% of original 285, drops 0)', 'datetime': '2021-11-14T23:56:24.731950', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:56:24: Doc2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 457 word corpus (100.0%% of original 457, drops 0)', 'datetime': '2021-11-14T23:56:24.731950', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:56:24: deleting the raw counts dictionary of 285 items\n",
      "INFO - 23:56:24: sample=0.001 downsamples 76 most-common words\n",
      "INFO - 23:56:24: Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 332.2836672601237 word corpus (72.7%% of prior 457)', 'datetime': '2021-11-14T23:56:24.733951', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "INFO - 23:56:24: estimated required memory for 285 words and 30 dimensions: 212180 bytes\n",
      "INFO - 23:56:24: resetting layer weights\n",
      "INFO - 23:56:24: Doc2Vec lifecycle event {'msg': 'training model with 3 workers on 285 vocabulary and 30 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2021-11-14T23:56:24.737955', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "INFO - 23:56:24: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:56:24: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:56:24: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:56:24: EPOCH - 1 : training on 457 raw words (339 effective words) took 0.0s, 247157 effective words/s\n",
      "INFO - 23:56:24: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:56:24: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:56:24: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:56:24: EPOCH - 2 : training on 457 raw words (343 effective words) took 0.0s, 307651 effective words/s\n",
      "INFO - 23:56:24: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:56:24: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:56:24: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:56:24: EPOCH - 3 : training on 457 raw words (336 effective words) took 0.0s, 263302 effective words/s\n",
      "INFO - 23:56:24: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:56:24: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:56:24: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:56:24: EPOCH - 4 : training on 457 raw words (350 effective words) took 0.0s, 258665 effective words/s\n",
      "INFO - 23:56:24: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:56:24: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:56:24: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:56:24: EPOCH - 5 : training on 457 raw words (334 effective words) took 0.0s, 238913 effective words/s\n",
      "INFO - 23:56:24: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:56:24: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:56:24: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:56:24: EPOCH - 6 : training on 457 raw words (331 effective words) took 0.0s, 226464 effective words/s\n",
      "INFO - 23:56:24: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:56:24: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:56:24: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:56:24: EPOCH - 7 : training on 457 raw words (336 effective words) took 0.0s, 243073 effective words/s\n",
      "INFO - 23:56:24: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:56:24: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:56:24: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:56:24: EPOCH - 8 : training on 457 raw words (343 effective words) took 0.0s, 245298 effective words/s\n",
      "INFO - 23:56:24: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:56:24: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:56:24: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:56:24: EPOCH - 9 : training on 457 raw words (342 effective words) took 0.0s, 210851 effective words/s\n",
      "INFO - 23:56:24: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 23:56:24: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 23:56:24: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 23:56:24: EPOCH - 10 : training on 457 raw words (343 effective words) took 0.0s, 265850 effective words/s\n",
      "INFO - 23:56:24: Doc2Vec lifecycle event {'msg': 'training on 4570 raw words (3397 effective words) took 0.0s, 108576 effective words/s', 'datetime': '2021-11-14T23:56:24.768984', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "INFO - 23:56:24: Doc2Vec lifecycle event {'params': 'Doc2Vec(dbow,d30,n5,s0.001,t3)', 'datetime': '2021-11-14T23:56:24.769984', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "# model = Word2Vec.load('basemodel.bin')\n",
    "# model.min_count = 30\n",
    "# model.alpha = 0.02\n",
    "# model.window = 3\n",
    "\n",
    "# name = []\n",
    "# print(len(model.wv.vectors))\n",
    "# for i in df_clean.CandidateID:\n",
    "#     for j in range(30):\n",
    "#         name.append(i)\n",
    "\n",
    "#     name = bigram[name]\n",
    "#     model.build_vocab(name, update=True, progress_per=30)\n",
    "#     model.train(name, total_examples=name.count, epochs=30)\n",
    "#     name = []\n",
    "# print(len(model.wv.vectors))\n",
    "documents = [TaggedDocument(doc, tags=[df_clean.CandidateID[i]]) for i, doc in enumerate(sentences)]\n",
    "model = Doc2Vec(documents, dm = 0, alpha=0.025, vector_size=30, min_alpha = 0.025, min_count = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-54-f882e909280d>:4: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  sims = model.docvecs.most_similar([new_vec])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('candidate_009', 0.9550108909606934),\n",
       " ('candidate_046', 0.953600287437439),\n",
       " ('candidate_070', 0.952103316783905),\n",
       " ('candidate_143', 0.9485359787940979)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = \"technology\".split()\n",
    "\n",
    "new_vec = model.infer_vector(tokens)\n",
    "sims = model.docvecs.most_similar([new_vec])\n",
    "sims"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "69653a312dbe931d74f7dcdd7e6244916ba6f541c5453c763d9189f9df467eeb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
